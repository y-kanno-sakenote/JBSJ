# -*- coding: utf-8 -*-
"""
é†¸é€ å”ä¼šèªŒ è«–æ–‡æ¤œç´¢ï¼ˆçµ±ä¸€UIç‰ˆï¼‰
- æ—¢å­˜æ©Ÿèƒ½ã¯å¤‰æ›´ãªã—
- è¿½åŠ : æ¤œç´¢çµæœã®è‘—è€…åã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€è‘—è€…ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆreadingãƒ™ãƒ¼ã‚¹ã® multiselectï¼‰ã¸è‡ªå‹•è¿½åŠ 
"""

import io, re, time, urllib.parse
from pathlib import Path

import pandas as pd
import requests
import streamlit as st

# -------------------- ãƒšãƒ¼ã‚¸è¨­å®š --------------------
st.set_page_config(page_title="è«–æ–‡æ¤œç´¢ï¼ˆçµ±ä¸€UIç‰ˆï¼‰", layout="wide")

# -------------------- å®šæ•° --------------------
KEY_COLS = [
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]
TARGET_ORDER = [
    "æ¸…é…’","ãƒ“ãƒ¼ãƒ«","ãƒ¯ã‚¤ãƒ³","ç„¼é…","ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«é£²æ–™","ç™ºé…µä¹³ãƒ»ä¹³è£½å“",
    "é†¤æ²¹","å‘³å™Œ","ç™ºé…µé£Ÿå“","è¾²ç”£ç‰©ãƒ»æœå®Ÿ","å‰¯ç”£ç‰©ãƒ»ãƒã‚¤ã‚ªãƒã‚¹","é…µæ¯ãƒ»å¾®ç”Ÿç‰©","ã‚¢ãƒŸãƒé…¸ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ª","ãã®ä»–"
]
TYPE_ORDER = [
    "å¾®ç”Ÿç‰©ãƒ»éºä¼å­é–¢é€£","é†¸é€ å·¥ç¨‹ãƒ»è£½é€ æŠ€è¡“","å¿œç”¨åˆ©ç”¨ãƒ»é£Ÿå“é–‹ç™º","æˆåˆ†åˆ†æãƒ»ç‰©æ€§è©•ä¾¡",
    "å“è³ªè©•ä¾¡ãƒ»å®˜èƒ½è©•ä¾¡","æ­´å²ãƒ»æ–‡åŒ–ãƒ»çµŒæ¸ˆ","å¥åº·æ©Ÿèƒ½ãƒ»æ „é¤ŠåŠ¹æœ","çµ±è¨ˆè§£æãƒ»ãƒ¢ãƒ‡ãƒ«åŒ–",
    "ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£","ä¿å­˜ãƒ»å®‰å®šæ€§","ãã®ä»–ï¼ˆç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰"
]

# -------------------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ --------------------
def norm_space(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    return re.sub(r"\s+", " ", s).strip()

def norm_key(s: str) -> str:
    return norm_space(s).lower()

AUTHOR_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+")
def split_authors(cell):
    if not cell: return []
    return [w.strip() for w in AUTHOR_SPLIT_RE.split(str(cell)) if w.strip()]

def split_multi(s):
    if not s: return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(s)) if w.strip()]

def tokens_from_query(q):
    q = norm_key(q)
    return [t for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", q) if t]

def fetch_csv(url: str) -> pd.DataFrame:
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    return pd.read_csv(io.BytesIO(r.content), encoding="utf-8")

def ensure_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df

def consolidate_authors_column(df: pd.DataFrame) -> pd.DataFrame:
    """è‘—è€…åˆ—ï¼šç©ºç™½ã§ã¯åˆ†å‰²ã›ãšã€åŒºåˆ‡ã‚Šè¨˜å·ã®ã¿ã§åˆ†å‰²â†’åŒåç•°è¡¨è¨˜ã‚’ä»£è¡¨è¡¨è¨˜ã«çµ±åˆ"""
    if "è‘—è€…" not in df.columns:
        return df
    df = df.copy()
    def unify(cell: str) -> str:
        names = split_authors(cell)
        seen = set()
        result = []
        for n in names:
            k = norm_key(n)
            if not k or k in seen:
                continue
            seen.add(k)
            result.append(n)
        return ", ".join(result)
    df["è‘—è€…"] = df["è‘—è€…"].astype(str).apply(unify)
    return df

def build_author_candidates(df: pd.DataFrame):
    rep = {}
    for v in df.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
        for name in split_authors(v):
            k = norm_key(name)
            if k and k not in rep:
                rep[k] = name
    return [rep[k] for k in sorted(rep.keys())]

def haystack(row, include_fulltext: bool):
    parts = [
        str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","")),
        str(row.get("è‘—è€…","")),
        str(row.get("file_name","")),
        " ".join(str(row.get(c,"")) for c in KEY_COLS if c in row),
    ]
    # æœ¬æ–‡æ¤œç´¢ã¯ç¾çŠ¶ãªã—ï¼ˆUIã‹ã‚‰å‰Šé™¤æ¸ˆã¿ï¼‰
    return norm_key(" \n ".join(parts))

def to_int_or_none(x):
    try: return int(str(x).strip())
    except Exception:
        m = re.search(r"\d+", str(x))
        return int(m.group()) if m else None

def order_by_template(values, template):
    """1) ãƒ†ãƒ³ãƒ—ãƒ¬ã®é † 2) æœªåè¼‰ã¯ã‚¢ãƒ«ãƒ•ã‚¡é † 3) ãã®ä»–ã¯æœ€å¾Œ"""
    vs = list(dict.fromkeys(values))
    tmpl_set = set(template)
    head = [v for v in template if v in vs and "ãã®ä»–" not in v]
    mid  = sorted([v for v in vs if v not in tmpl_set and "ãã®ä»–" not in v])
    tail = [v for v in template if v in vs and "ãã®ä»–" in v] + \
           [v for v in vs if ("ãã®ä»–" in v and v not in template)]
    return head + mid + tail

def make_visible_cols(df: pd.DataFrame) -> list[str]:
    """df ã®åˆ—ã‹ã‚‰ã€ç›¸å¯¾PASS/çµ‚äº†ãƒšãƒ¼ã‚¸/file_path/num_pages/file_nameã€ã¨
       ã€llm_keywords ä»¥é™ã®å…¨åˆ—ã€ã‚’éè¡¨ç¤ºå¯¾è±¡ã«ã—ã¦ã€è¡¨ç¤ºåˆ—ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    base_hide = {"ç›¸å¯¾PASS", "çµ‚äº†ãƒšãƒ¼ã‚¸", "file_path", "num_pages", "file_name"}
    cols = [str(c) for c in df.columns]
    hide = set(c for c in cols if c in base_hide)
    if "llm_keywords" in cols:
        idx = cols.index("llm_keywords")
        hide.update(cols[idx:])
    return [c for c in cols if c not in hide]

def make_row_id(row):
    no = str(row.get("No.", "")).strip()
    if no and no.lower() not in {"none", "nan"}:
        return f"NO:{no}"
    ttl = str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«", "")).strip()
    yr  = str(row.get("ç™ºè¡Œå¹´", "")).strip()
    return f"T:{ttl}|Y:{yr}"

# === è‘—è€…èª­ã¿CSVã®ãƒ­ãƒ¼ãƒ€ï¼ˆauthors_readings.csvï¼‰ ===
AUTHORS_CSV_PATH = Path("data/authors_readings.csv")

@st.cache_data(ttl=600, show_spinner=False)
def load_authors_readings(path: Path) -> pd.DataFrame | None:
    try:
        df_a = pd.read_csv(path, encoding="utf-8")
        df_a.columns = [str(c).strip() for c in df_a.columns]
        if not {"author", "reading"}.issubset(set(df_a.columns)):
            return None
        df_a["author"]  = df_a["author"].astype(str).str.strip()
        df_a["reading"] = df_a["reading"].astype(str).str.strip()
        df_a = df_a[(df_a["author"]!="") & (df_a["reading"]!="")]
        df_a = df_a.drop_duplicates(subset=["reading"], keep="first")
        return df_a
    except Exception:
        return None

# -------------------- ã‚¿ã‚¤ãƒˆãƒ« --------------------
st.title("é†¸é€ å”ä¼šèªŒã€€è«–æ–‡æ¤œç´¢")

# -------------------- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ --------------------
DEMO_CSV_PATH = Path("data/keywords_summary4.csv")
SECRET_URL = st.secrets.get("GSHEET_CSV_URL", "")

@st.cache_data(ttl=600, show_spinner=False)
def load_local_csv(path: Path) -> pd.DataFrame:
    return ensure_cols(pd.read_csv(path, encoding="utf-8"))

@st.cache_data(ttl=600, show_spinner=False)
def load_url_csv(url: str) -> pd.DataFrame:
    return ensure_cols(fetch_csv(url))

with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    st.caption("â€» ã¾ãšã¯ãƒ‡ãƒ¢ç”¨CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã€‚URL/ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šã§ä¸Šæ›¸ãã§ãã¾ã™ã€‚")
    use_demo = st.toggle("ãƒ‡ãƒ¢CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹", value=True)
    url = st.text_input("å…¬é–‹CSVã®URLï¼ˆGoogleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ output=csvï¼‰", value=SECRET_URL)
    up  = st.file_uploader("CSVã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿", type=["csv"])
    load_clicked = st.button("èª­ã¿è¾¼ã¿ï¼ˆURL/ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆï¼‰", type="primary", key="load_btn")

df = None
err = None
try:
    if load_clicked:
        if up is not None:
            df = ensure_cols(pd.read_csv(up, encoding="utf-8"))
            st.toast("ãƒ­ãƒ¼ã‚«ãƒ«CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        elif url.strip():
            df = load_url_csv(url.strip())
            st.toast("URLã®CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            st.warning("URL ã¾ãŸã¯ CSV ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    elif use_demo and DEMO_CSV_PATH.exists():
        df = load_local_csv(DEMO_CSV_PATH)
        st.caption(f"âœ… ãƒ‡ãƒ¢CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ä¸­: {DEMO_CSV_PATH}")
    elif SECRET_URL:
        df = load_url_csv(SECRET_URL)
        st.caption("âœ… Secretsã®URLã‹ã‚‰è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ä¸­")
except Exception as e:
    err = e

if df is None:
    if err:
        st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {err}")
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ CSV ã‚’æŒ‡å®šã™ã‚‹ã‹ã€ãƒ‡ãƒ¢CSVã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# -------------------- å¹´ãƒ»å·»ãƒ»å·ãƒ•ã‚£ãƒ«ã‚¿ --------------------
st.subheader("å¹´ãƒ»å·»ãƒ»å·ãƒ•ã‚£ãƒ«ã‚¿")
year_vals = pd.to_numeric(df.get("ç™ºè¡Œå¹´", pd.Series(dtype=str)), errors="coerce")
if year_vals.notna().any():
    ymin_all, ymax_all = int(year_vals.min()), int(year_vals.max())
else:
    ymin_all, ymax_all = 1980, 2025

c_y, c_v, c_i = st.columns([1, 1, 1])
with c_y:
    y_from, y_to = st.slider(
        "ç™ºè¡Œå¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin_all, max_value=ymax_all,
        value=(ymin_all, ymax_all)
    )
with c_v:
    vol_candidates = sorted({v for v in (df.get("å·»æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
    vols_sel = st.multiselect("å·»ï¼ˆè¤‡æ•°é¸æŠï¼‰", vol_candidates, default=[])
with c_i:
    iss_candidates = sorted({v for v in (df.get("å·æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
    issues_sel = st.multiselect("å·ï¼ˆè¤‡æ•°é¸æŠï¼‰", iss_candidates, default=[])

# -------------------- æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ --------------------
st.subheader("æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿")
# 1æ®µç›®ï¼šå¯¾è±¡ç‰© / ç ”ç©¶ã‚¿ã‚¤ãƒ—
c_tg, c_tp = st.columns([1.2, 1.2])
with c_tg:
    raw_targets = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
    targets_all = order_by_template(list(raw_targets), TARGET_ORDER)
    targets_sel = st.multiselect("å¯¾è±¡ç‰©ï¼ˆè¤‡æ•°é¸æŠï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", targets_all, default=[])
with c_tp:
    raw_types = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
    types_all = order_by_template(list(raw_types), TYPE_ORDER)
    types_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆè¤‡æ•°é¸æŠï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", types_all, default=[])

# 2æ®µç›®ï¼šè‘—è€…ï¼ˆå³ã«ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«ï¼‰
c_a1, c_a2 = st.columns([2.0, 1.2])
with c_a1:
    # è‘—è€…ï¼ˆèª­ã¿ã§æ¤œç´¢ãƒ»è¡¨ç¤ºã¯æ¼¢å­—ï¼‹èª­ã¿ï¼‰
    adf = load_authors_readings(AUTHORS_CSV_PATH)
    authors_sel = []
    authors_sel_readings_default = st.session_state.get("authors_sel_readings", [])
    if adf is not None:
        # ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«ï¼ˆã‚ã‹ã•ãŸãª/è‹±å­—ï¼‰
        if "author_initial" not in st.session_state:
            st.session_state.author_initial = "ã™ã¹ã¦"
        initials = ["ã™ã¹ã¦","ã‚","ã‹","ã•","ãŸ","ãª","ã¯","ã¾","ã‚„","ã‚‰","ã‚","è‹±å­—"]

        # å€™è£œç”Ÿæˆ
        cand = adf.copy()

        GOJUON = {
            "ã‚": "ã‚ã„ã†ãˆãŠãŒããã’ã”",
            "ã‹": "ã‹ããã‘ã“ãŒããã’ã”",
            "ã•": "ã•ã—ã™ã›ãã–ã˜ãšãœã",
            "ãŸ": "ãŸã¡ã¤ã¦ã¨ã ã¢ã¥ã§ã©",
            "ãª": "ãªã«ã¬ã­ã®",
            "ã¯": "ã¯ã²ãµã¸ã»ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½",
            "ã¾": "ã¾ã¿ã‚€ã‚ã‚‚",
            "ã‚„": "ã‚„ã‚†ã‚ˆ",
            "ã‚‰": "ã‚‰ã‚Šã‚‹ã‚Œã‚",
            "ã‚": "ã‚ã‚’ã‚“",
        }
        def kata_to_hira(s: str) -> str:
            out = []
            for ch in str(s):
                code = ord(ch)
                if 0x30A1 <= code <= 0x30F6:
                    out.append(chr(code - 0x60))
                else:
                    out.append(ch)
            return "".join(out)

        def is_roman_head(s: str) -> bool:
            return bool(re.match(r"[A-Za-z]", str(s or "")))

        def hira_head(s: str) -> str | None:
            s = str(s or "")
            if not s: return None
            h = kata_to_hira(s)[0]
            return h

        ini = st.session_state.author_initial

        # ä¸¦ã³æ›¿ãˆï¼šã²ã‚‰ãŒãªâ†’ã‚«ã‚¿ã‚«ãƒŠâ†’è‹±å­—
        AIUEO_ORDER = "ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“"
        def sort_tuple_reading(r):
            r = str(r or "")
            if not r: return (3, 999, r)
            ch = r[0]
            if re.match(r"[A-Za-z]", ch):
                return (2, 999, ch.lower())
            if re.match(r"[\u30A0-\u30FF]", ch):
                return (1, 999, r)
            idx = AIUEO_ORDER.find(ch)
            if idx == -1: idx = 998
            return (0, idx, r)

        # ãƒ©ã‚¸ã‚ªã§ç¯„å›²çµã‚Šè¾¼ã¿
        def filter_by_initial(df_cand: pd.DataFrame, initial: str) -> pd.DataFrame:
            if initial == "è‹±å­—":
                m = df_cand["reading"].astype(str).str.match(r"[A-Za-z]")
                return df_cand[m]
            if initial == "ã™ã¹ã¦":
                return df_cand
            allowed = set(GOJUON.get(initial, ""))
            def in_row(s):
                if not s or is_roman_head(s):
                    return False
                h = hira_head(s)
                return bool(h and h in allowed)
            return df_cand[df_cand["reading"].apply(in_row)]

        cand = filter_by_initial(cand, ini)
        # ä¸¦ã¹æ›¿ãˆ
        _tmp = cand["reading"].apply(lambda r: pd.Series(sort_tuple_reading(r), index=["_grp","_key","_sub"]))
        cand = pd.concat([cand.reset_index(drop=True), _tmp], axis=1)\
                 .sort_values(by=["_grp","_key","_sub"], kind="mergesort")\
                 .drop(columns=["_grp","_key","_sub"]).reset_index(drop=True)

        reading2author = dict(zip(cand["reading"], cand["author"]))
        options_readings = list(reading2author.keys())

        authors_sel_readings = st.multiselect(
            "è‘—è€…ï¼ˆèª­ã¿ã§æ¤œç´¢å¯ / è¡¨ç¤ºã¯æ¼¢å­—ï¼‹èª­ã¿ï¼‰",
            options=options_readings,
            format_func=lambda r: f"{reading2author.get(r, r)}ï½œ{r}",
            placeholder="ä¾‹ï¼šã‚„ã¾ã  / ã•ã¨ã† / ãŸã‹ã¯ã— ...",
            default=[r for r in authors_sel_readings_default if r in options_readings]
        )
        authors_sel = sorted({reading2author[r] for r in authors_sel_readings}) if authors_sel_readings else []
        st.session_state["authors_sel_readings"] = authors_sel_readings
        st.session_state["authors_sel"] = authors_sel
        st.session_state["reading2author_latest_map"] = reading2author  # ã‚¯ãƒªãƒƒã‚¯æ™‚ã®é€†å¼•ãã«ä½¿ã†
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        authors_all = build_author_candidates(df)
        authors_sel = st.multiselect("è‘—è€…", authors_all, default=[])

with c_a2:
    st.caption("è‘—è€…ã®èª­ã¿ãƒ»ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«")
    initials = ["ã™ã¹ã¦","ã‚","ã‹","ã•","ãŸ","ãª","ã¯","ã¾","ã‚„","ã‚‰","ã‚","è‹±å­—"]
    if "author_initial" not in st.session_state:
        st.session_state.author_initial = "ã™ã¹ã¦"
    st.session_state.author_initial = st.radio(
        "è‘—è€…ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«é¸æŠ",
        initials,
        horizontal=True,
        index=initials.index(st.session_state.author_initial) if st.session_state.author_initial in initials else 0,
        key="author_initial_radio"
    )

# 3æ®µç›®ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
c_kw1, c_kw2 = st.columns([3, 1])
with c_kw1:
    kw_query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½/ã‚«ãƒ³ãƒã§è¤‡æ•°å¯ï¼‰", value="")
with c_kw2:
    kw_mode = st.radio("ä¸€è‡´æ¡ä»¶", ["OR", "AND"], index=0, horizontal=True, key="kw_mode")

# -------------------- ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ --------------------
def apply_filters(_df: pd.DataFrame) -> pd.DataFrame:
    df2 = _df.copy()
    if "ç™ºè¡Œå¹´" in df2.columns:
        y = pd.to_numeric(df2["ç™ºè¡Œå¹´"], errors="coerce")
        df2 = df2[(y >= y_from) & (y <= y_to) | y.isna()]
    if "å·»æ•°" in df2.columns and vols_sel:
        df2 = df2[df2["å·»æ•°"].map(to_int_or_none).isin(set(vols_sel))]
    if "å·æ•°" in df2.columns and issues_sel:
        df2 = df2[df2["å·æ•°"].map(to_int_or_none).isin(set(issues_sel))]
    if authors_sel and "è‘—è€…" in df2.columns:
        sel = {norm_key(a) for a in authors_sel}
        def hit_author(v): return any(norm_key(x) in sel for x in split_authors(v))
        df2 = df2[df2["è‘—è€…"].apply(hit_author)]
    if targets_sel and "å¯¾è±¡ç‰©_top3" in df2.columns:
        t_norm = [norm_key(t) for t in targets_sel]
        df2 = df2[df2["å¯¾è±¡ç‰©_top3"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]
    if types_sel and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in df2.columns:
        t_norm = [norm_key(t) for t in types_sel]
        df2 = df2[df2["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]
    toks = tokens_from_query(kw_query)
    if toks:
        def hit_kw(row):
            hs = haystack(row, include_fulltext=False)
            return all(t in hs for t in toks) if kw_mode == "AND" else any(t in hs for t in toks)
        df2 = df2[df2.apply(hit_kw, axis=1)]
    return df2

filtered = apply_filters(df)

# -------------------- æ¤œç´¢çµæœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ—¢å­˜ãã®ã¾ã¾ï¼‰ --------------------
st.markdown("### æ¤œç´¢çµæœ")
st.caption(f"{len(filtered)} / {len(df)} ä»¶")

visible_cols = make_visible_cols(filtered)
disp = filtered.loc[:, visible_cols].copy()
disp["_row_id"] = disp.apply(make_row_id, axis=1)

# summary ã®å¯èª­æ€§ï¼ˆçœç•¥ã—ã¦ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ï¼‰ï¼šæ—¢å­˜ã®åˆ—å 'summary' ãŒã‚ã‚Œã°åŠ å·¥
if "summary" in disp.columns:
    def clip_sum(s, n=120):
        s = str(s or "")
        return s if len(s) <= n else (s[:n] + "â€¦")
    disp["_summary_tip"] = disp["summary"].astype(str)
    disp["summary"] = disp["summary"].apply(clip_sum)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼šãŠæ°—ã«å…¥ã‚Šé›†åˆï¼ã‚¿ã‚°è¾æ›¸
if "favs" not in st.session_state:
    st.session_state.favs = set()
if "fav_tags" not in st.session_state:
    st.session_state.fav_tags = {}   # row_id -> set(tags)

# ãƒ¡ã‚¤ãƒ³è¡¨ï¼šãŠæ°—ã«å…¥ã‚Šãƒã‚§ãƒƒã‚¯åˆ—
disp["â˜…"] = disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)

# LinkColumn è¨­å®š
column_config = {
    "â˜…": st.column_config.CheckboxColumn("â˜…", help="æ°—ã«ãªã‚‹è«–æ–‡ã«ãƒã‚§ãƒƒã‚¯/è§£é™¤", default=False, width="small"),
}
if "HPãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
    column_config["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", help="å¤–éƒ¨ã‚µã‚¤ãƒˆã¸ç§»å‹•", display_text="HP")
if "PDFãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
    column_config["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", help="PDFã‚’é–‹ã", display_text="PDF")
if "summary" in disp.columns and "_summary_tip" in disp.columns:
    column_config["summary"] = st.column_config.TextColumn("summaryï¼ˆãƒ›ãƒãƒ¼ã§å…¨æ–‡ï¼‰", help="", disabled=True)

display_order = ["â˜…"] + [c for c in disp.columns if c not in ["â˜…", "_row_id", "_summary_tip"]] + ["_row_id"]

st.subheader("è«–æ–‡ãƒªã‚¹ãƒˆ")
with st.form("main_table_form", clear_on_submit=False):
    edited_main = st.data_editor(
        disp[display_order],
        key="main_editor",
        use_container_width=True,
        hide_index=True,
        column_config=column_config,
        disabled=[c for c in display_order if c != "â˜…"],
        height=520,
        num_rows="fixed",
    )
    apply_main = st.form_submit_button("ãƒã‚§ãƒƒã‚¯ã—ãŸè«–æ–‡ã‚’ãŠæ°—ã«å…¥ã‚Šãƒªã‚¹ãƒˆã«è¿½åŠ ", use_container_width=True)

if apply_main:
    subset_ids_main = set(disp["_row_id"].tolist())
    checked_subset_main = set(edited_main.loc[edited_main["â˜…"] == True, "_row_id"].tolist())
    st.session_state.favs = (st.session_state.favs - subset_ids_main) | checked_subset_main

# -------------------- è¿½åŠ ï¼šè‘—è€…ã‚¯ãƒªãƒƒã‚¯è£œåŠ©ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆè‘—è€…åã‚’ãƒªãƒ³ã‚¯åŒ–ï¼‰ --------------------
def _author_links_cell(authors_cell: str) -> str:
    names = split_authors(authors_cell)
    links = []
    for a in names:
        a_clean = a.strip()
        if not a_clean:
            continue
        href = "?add_author=" + urllib.parse.quote(a_clean)
        links.append(f'<a href="{href}" style="color:#1a73e8; text-decoration:underline;">{a_clean}</a>')
    return " / ".join(links) if links else ""

# è‘—è€…ãƒªãƒ³ã‚¯ã®ã¿ã®è»½é‡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆNo. / è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ« / è‘—è€…ãƒªãƒ³ã‚¯ï¼‰
mini = filtered.copy()
keep_cols = []
if "No." in mini.columns: keep_cols.append("No.")
if "è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«" in mini.columns: keep_cols.append("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«")
if "è‘—è€…" in mini.columns: keep_cols.append("è‘—è€…")
mini = mini.loc[:, keep_cols].copy() if keep_cols else pd.DataFrame(columns=["è‘—è€…"])

if not mini.empty and "è‘—è€…" in mini.columns:
    mini["è‘—è€…ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ ï¼‰"] = mini["è‘—è€…"].apply(_author_links_cell)
    show_cols = [c for c in ["No.","è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","è‘—è€…ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ ï¼‰"] if c in mini.columns]
    st.markdown("##### è‘—è€…ã‚¯ãƒªãƒƒã‚¯ç”¨ï¼ˆè£œåŠ©è¡¨ç¤ºï¼‰")
    st.markdown(
        mini[show_cols].to_html(escape=False, index=False),
        unsafe_allow_html=True
    )
    st.caption("â€» ä¸Šã®ã€Œè«–æ–‡ãƒªã‚¹ãƒˆã€ã¯å¾“æ¥é€šã‚Šç·¨é›†å¯ã€ã“ã¡ã‚‰ã¯â€œè‘—è€…ã‚¯ãƒªãƒƒã‚¯ã§ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ â€å°‚ç”¨ã®è£œåŠ©è¡¨ç¤ºã§ã™ã€‚")

# ---- ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰è‘—è€…è¿½åŠ ï¼ˆreadingã¸é€†å¼•ãã—ã¦ multiselect ã«åæ˜ ï¼‰ ----
if "add_author" in st.query_params:
    add_name = st.query_params.get("add_author")
    # æœ€æ–°ã®å€™è£œãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆreading -> authorï¼‰ã‚’å–å¾—ï¼ˆç„¡ã„å ´åˆã¯ authors_readings.csv ã‹ã‚‰å¾©å…ƒï¼‰
    reading2author = st.session_state.get("reading2author_latest_map")
    if reading2author is None and adf is not None:
        reading2author = dict(zip(adf["reading"], adf["author"]))

    added_readings = []
    if reading2author:
        # author -> reading ã®é€†å¼•ãï¼ˆè¤‡æ•°ä¸€è‡´ã¯ã™ã¹ã¦ï¼‰
        for r, a in reading2author.items():
            if a == add_name:
                added_readings.append(r)

    if added_readings:
        cur = st.session_state.get("authors_sel_readings", [])
        new_list = list(dict.fromkeys(cur + [r for r in added_readings if r not in cur]))
        st.session_state["authors_sel_readings"] = new_list
        # authors_sel ã‚‚åŒæœŸ
        st.session_state["authors_sel"] = sorted({reading2author[r] for r in new_list})
        # ã‚¯ã‚¨ãƒªã‚’æ¶ˆã—ã¦åæ˜ 
        st.query_params.clear()
        st.rerun()
    else:
        # é€†å¼•ãã§ããªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆã‚¯ã‚¨ãƒªã ã‘æ¶ˆã™ï¼‰
        st.query_params.clear()

# -------------------- ãŠæ°—ã«å…¥ã‚Šä¸€è¦§ --------------------
c1, c2 = st.columns([6, 1])
with c1:
    st.subheader(f"â­ ãŠæ°—ã«å…¥ã‚Šï¼ˆ{len(st.session_state.favs)} ä»¶ï¼‰")
with c2:
    if st.button("âŒ å…¨ã¦å¤–ã™", key="clear_favs_header", use_container_width=True):
        st.session_state.favs = set()
        st.rerun()

visible_cols_full = make_visible_cols(df)
fav_disp_full = df.loc[:, visible_cols_full].copy()
fav_disp_full["_row_id"] = fav_disp_full.apply(make_row_id, axis=1)
fav_disp = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()

def tags_str_for(rid: str) -> str:
    s = st.session_state.fav_tags.get(rid, set())
    return ", ".join(sorted(s)) if s else ""

if not fav_disp.empty:
    fav_disp["â˜…"] = fav_disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)
    fav_disp["tags"] = fav_disp["_row_id"].apply(tags_str_for)

    fav_display_order = ["â˜…"] + [c for c in fav_disp.columns if c not in ["â˜…", "_row_id"]] + ["_row_id"]

    fav_column_config = {
        "â˜…": st.column_config.CheckboxColumn("â˜…", help="ãƒã‚§ãƒƒã‚¯ã§è§£é™¤/è¿½åŠ ï¼ˆä¸‹ã®ãƒœã‚¿ãƒ³ã§åæ˜ ï¼‰", default=True, width="small"),
        "tags": st.column_config.TextColumn("tagsï¼ˆã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", help="ä¾‹: æ¸…é…’, ä¹³é…¸èŒ"),
    }
    if "HPãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
        fav_column_config["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", display_text="HP")
    if "PDFãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
        fav_column_config["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", display_text="PDF")

    with st.form("fav_table_form", clear_on_submit=False):
        fav_edited = st.data_editor(
            fav_disp[fav_display_order],
            key="fav_editor",
            use_container_width=True,
            hide_index=True,
            column_config=fav_column_config,
            disabled=[c for c in fav_display_order if c not in ["â˜…", "tags"]],
            height=420,
            num_rows="fixed",
        )
        apply_fav = st.form_submit_button("ãŠæ°—ã«å…¥ã‚Šã®å¤‰æ›´ï¼ˆâ˜…/tagsï¼‰ã‚’æ›´æ–°", use_container_width=True)

    if apply_fav:
        subset_ids_fav = set(fav_disp["_row_id"].tolist())
        fav_checked_subset = set(fav_edited.loc[fav_edited["â˜…"] == True, "_row_id"].tolist())
        st.session_state.favs = (st.session_state.favs - subset_ids_fav) | fav_checked_subset

        def parse_tags(s):
            if not isinstance(s, str): s = str(s or "")
            parts = [t.strip() for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", s) if t.strip()]
            return set(parts)
        for _, r in fav_edited.iterrows():
            rid = r["_row_id"]
            tag_set = parse_tags(r.get("tags", ""))
            if tag_set:
                st.session_state.fav_tags[rid] = tag_set
            elif rid in st.session_state.fav_tags:
                del st.session_state.fav_tags[rid]

        st.success("ãŠæ°—ã«å…¥ã‚Šï¼ˆâ˜…/tagsï¼‰ã‚’åæ˜ ã—ã¾ã—ãŸ")
        st.rerun()
else:
    st.info("ãŠæ°—ã«å…¥ã‚Šã¯æœªé¸æŠã§ã™ã€‚ä¸Šã®è¡¨ã®ã€â˜…ã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰åæ˜ ã—ã¦ãã ã•ã„ã€‚")

# -------------------- ã‚¿ã‚°ã§ãŠæ°—ã«å…¥ã‚Šã‚’çµã‚Šè¾¼ã¿ï¼ˆæŠ˜ã‚Šç•³ã¿ï¼‰ --------------------
with st.expander("ğŸ” ã‚¿ã‚°ã§ãŠæ°—ã«å…¥ã‚Šã‚’çµã‚Šè¾¼ã¿ï¼ˆAND/ORï¼‰", expanded=False):
    tag_query = st.text_input("ã‚¿ã‚°æ¤œç´¢ï¼ˆã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key="tag_query")
    tag_mode = st.radio("ä¸€è‡´æ¡ä»¶", ["OR", "AND"], index=0, horizontal=True, key="tag_mode")

    fav_disp_for_filter = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()
    if tag_query.strip():
        tags = [t.strip() for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", tag_query) if t.strip()]
        def match_tags_row(row):
            row_tags = st.session_state.fav_tags.get(row["_row_id"], set())
            return all(t in row_tags for t in tags) if tag_mode == "AND" else any(t in row_tags for t in tags)
        fav_disp_for_filter = fav_disp_for_filter[fav_disp_for_filter.apply(match_tags_row, axis=1)]

    def tags_str_for_filter(rid: str) -> str:
        s = st.session_state.fav_tags.get(rid, set())
        return ", ".join(sorted(s)) if s else ""
    fav_disp_for_filter["tags"] = fav_disp_for_filter["_row_id"].apply(tags_str_for_filter)

    show_cols = ["No.","ç™ºè¡Œå¹´","å·»æ•°","å·æ•°","è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","è‘—è€…","å¯¾è±¡ç‰©_top3","ç ”ç©¶ã‚¿ã‚¤ãƒ—","HPãƒªãƒ³ã‚¯å…ˆ","PDFãƒªãƒ³ã‚¯å…ˆ","tags"]
    show_cols = [c for c in show_cols if c in fav_disp_for_filter.columns]
    st.dataframe(fav_disp_for_filter[show_cols], use_container_width=True, hide_index=True)

# -------------------- ä¸‹éƒ¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆCSVå‡ºåŠ›ï¼š2ç¨®é¡ï¼‰ --------------------
st.caption(
    f"ç¾åœ¨ã®ãŠæ°—ã«å…¥ã‚Šï¼š{len(st.session_state.favs)} ä»¶ / "
    f"ã‚¿ã‚°æ•°ï¼š{len({t for s in st.session_state.fav_tags.values() for t in s})} ç¨®"
)

filtered_export_df = disp.drop(columns=["â˜…", "_row_id", "_summary_tip"], errors="ignore")

fav_export = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()
def _tags_join(rid: str) -> str:
    s = st.session_state.fav_tags.get(rid, set())
    return ", ".join(sorted(s)) if s else ""
fav_export["tags"] = fav_export["_row_id"].map(_tags_join)
fav_export = fav_export.drop(columns=["_row_id"], errors="ignore")

c_dl1, c_dl2 = st.columns(2)
with c_dl1:
    st.download_button(
        "ğŸ“¥ çµã‚Šè¾¼ã¿çµæœã‚’CSVå‡ºåŠ›ï¼ˆè¡¨ç¤ºåˆ—ã®ã¿ï¼‰",
        data=filtered_export_df.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"filtered_{time.strftime('%Y%m%d')}.csv",
        mime="text/csv",
        use_container_width=True
    )
with c_dl2:
    st.download_button(
        "â­ ãŠæ°—ã«å…¥ã‚Šã‚’CSVå‡ºåŠ›ï¼ˆtagsä»˜ãï¼‰",
        data=fav_export.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"favorites_{time.strftime('%Y%m%d')}.csv",
        mime="text/csv",
        use_container_width=True,
        disabled=fav_export.empty
    )