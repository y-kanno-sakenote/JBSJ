# -*- coding: utf-8 -*-
"""
è«–æ–‡æ¤œç´¢ï¼ˆçµ±ä¸€UIç‰ˆï¼šãŠæ°—ã«å…¥ã‚Šã«ã‚¿ã‚°ã‚’â€œè¡¨ã§ç›´æ¥å…¥åŠ›â€ï¼‰ï¼‹ åˆ†æã‚¿ãƒ–

æ©Ÿèƒ½ï¼ˆæ¤œç´¢ã‚¿ãƒ–ï¼šå¾“æ¥ã©ãŠã‚Šï¼‰
- ç™ºè¡Œå¹´ãƒ¬ãƒ³ã‚¸ã€å·»ãƒ»å·ï¼ˆè¤‡æ•°é¸æŠï¼‰ã€è‘—è€…ï¼ˆæ­£è¦åŒ–ãƒ»è¤‡æ•°é¸æŠ/èª­ã¿ã®é ­æ–‡å­—ãƒ©ã‚¸ã‚ªï¼‹ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼‰ã€
  å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»è¤‡æ•°é¸æŠï¼‰
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ AND/OR æ¤œç´¢ï¼ˆç©ºç™½/ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
- æ¤œç´¢çµæœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆä¸è¦åˆ—ã®éè¡¨ç¤ºã€HP/PDF ã®ãƒªãƒ³ã‚¯åŒ–ã€â˜…ã§ãŠæ°—ã«å…¥ã‚Šï¼‰
- ãŠæ°—ã«å…¥ã‚Šä¸€è¦§ï¼ˆå¸¸è¨­ãƒ»â˜…ã§è§£é™¤/è¿½åŠ ï¼‰ã€tags åˆ—ã‚’ç›´æ¥ç·¨é›†ã€âŒ å…¨ã¦å¤–ã™
- summaries.csv ã® summary ã‚’ã€Œè‘—è€…ã€ã®å³åˆ—ã«è¡¨ç¤º

æ©Ÿèƒ½ï¼ˆåˆ†æã‚¿ãƒ–ï¼šæ–°è¦ï¼‰
- å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ï¼šæ¤œç´¢çµæœ or å…¨ä»¶ ã‚’é¸æŠ
- è‘—è€…å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆåŒä¸€è«–æ–‡ã®è‘—è€…åŒå£«ã‚’ã‚¨ãƒƒã‚¸ï¼‰
- ä¸­å¿ƒæ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆDegree / Betweenness / Eigenvectorï¼‰
- PyVis ã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–ï¼ˆæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã¯ matplotlib ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
"""

import io, re, time, math
import pandas as pd
import requests
import streamlit as st
from pathlib import Path

# è¿½åŠ ï¼šåˆ†æç”¨
import itertools
import networkx as nx
import numpy as np

# PyVis ã¯ä»»æ„
try:
    from pyvis.network import Network
    _HAS_PYVIS = True
except Exception:
    _HAS_PYVIS = False

# -------------------- ãƒšãƒ¼ã‚¸è¨­å®š --------------------
st.set_page_config(page_title="è«–æ–‡æ¤œç´¢ï¼ˆçµ±ä¸€UIï¼‹åˆ†æï¼‰", layout="wide")

# -------------------- ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼ˆè‘—è€…ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³å¼·åŒ–ç‰ˆï¼šå¾“æ¥CSSï¼‰ --------------------
st.markdown(
    """
    <style>
    .stTextInput input, .stNumberInput input, textarea {
      background-color: #e0e0e0 !important;
      color: #000 !important;
      border: 1px solid #666 !important;
      border-radius: 6px !important;
      padding: 4px 8px !important;
    }
    .stMultiSelect div[data-baseweb="select"],
    .stSelectbox  div[data-baseweb="select"] {
      background-color: #e0e0e0 !important;
      border: 1px solid #666 !important;
      border-radius: 6px !important;
    }
    div[data-baseweb="select"] > div { background: transparent !important; }
    div[data-baseweb="select"] span { color: #000 !important; }
    div[data-baseweb="select"] svg  { color: #000 !important; fill: #000 !important; }
    div[data-baseweb="tag"] {
      background: #d5d5d5 !important;
      color: #000 !important;
      border-radius: 12px !important;
    }
    input:focus, textarea:focus,
    .stTextInput input:focus, .stNumberInput input:focus {
      border: 2px solid #1a73e8 !important;
      box-shadow: 0 0 4px #1a73e8 !important;
      outline: none !important;
    }
    .stMultiSelect div[data-baseweb="select"]:focus-within,
    .stSelectbox  div[data-baseweb="select"]:focus-within {
      border: 2px solid #1a73e8 !important;
      box-shadow: 0 0 4px #1a73e8 !important;
    }
    .stMultiSelect input:focus,
    .stSelectbox  input:focus {
      border: none !important;
      box-shadow: none !important;
      outline: none !important;
    }
    ul[role="listbox"] {
      background: #f5f5f5 !important;
      border: 1px solid #666 !important;
      max-height: 70vh !important;
      min-height: 360px !important;
      overflow-y: auto !important;
      padding-right: 6px !important;
      scrollbar-width: auto;
      scrollbar-color: #555 #e9e9e9;
    }
    li[role="option"] {
      padding: 8px 12px !important;
      line-height: 1.4 !important;
      font-size: 0.95rem !important;
    }
    li[role="option"]:hover,
    li[role="option"][aria-selected="true"] {
      background: #e0e0e0 !important;
      color: #000 !important;
    }
    ul[role="listbox"]::-webkit-scrollbar { width: 16px; }
    ul[role="listbox"]::-webkit-scrollbar-track { background: #e9e9e9; border-radius: 8px; }
    ul[role="listbox"]::-webkit-scrollbar-thumb {
      background: #555; border-radius: 8px; border: 3px solid #e9e9e9;
    }
    ul[role="listbox"]::-webkit-scrollbar-thumb:hover { background: #333; }
    </style>
    """,
    unsafe_allow_html=True
)

# -------------------- å®šæ•°ï¼ˆå¾“æ¥ï¼‰ --------------------
KEY_COLS = [
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]
TARGET_ORDER = [
    "æ¸…é…’","ãƒ“ãƒ¼ãƒ«","ãƒ¯ã‚¤ãƒ³","ç„¼é…","ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«é£²æ–™","ç™ºé…µä¹³ãƒ»ä¹³è£½å“",
    "é†¤æ²¹","å‘³å™Œ","ç™ºé…µé£Ÿå“","è¾²ç”£ç‰©ãƒ»æœå®Ÿ","å‰¯ç”£ç‰©ãƒ»ãƒã‚¤ã‚ªãƒã‚¹","é…µæ¯ãƒ»å¾®ç”Ÿç‰©","ã‚¢ãƒŸãƒé…¸ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ª","ãã®ä»–"
]
TYPE_ORDER = [
    "å¾®ç”Ÿç‰©ãƒ»éºä¼å­é–¢é€£","é†¸é€ å·¥ç¨‹ãƒ»è£½é€ æŠ€è¡“","å¿œç”¨åˆ©ç”¨ãƒ»é£Ÿå“é–‹ç™º","æˆåˆ†åˆ†æãƒ»ç‰©æ€§è©•ä¾¡",
    "å“è³ªè©•ä¾¡ãƒ»å®˜èƒ½è©•ä¾¡","æ­´å²ãƒ»æ–‡åŒ–ãƒ»çµŒæ¸ˆ","å¥åº·æ©Ÿèƒ½ãƒ»æ „é¤ŠåŠ¹æœ","çµ±è¨ˆè§£æãƒ»ãƒ¢ãƒ‡ãƒ«åŒ–",
    "ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£","ä¿å­˜ãƒ»å®‰å®šæ€§","ãã®ä»–ï¼ˆç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰"
]

# -------------------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå¾“æ¥ï¼‰ --------------------
def norm_space(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    return re.sub(r"\s+", " ", s).strip()

def norm_key(s: str) -> str:
    return norm_space(s).lower()

AUTHOR_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+")
def split_authors(cell):
    if not cell: return []
    return [w.strip() for w in AUTHOR_SPLIT_RE.split(str(cell)) if w.strip()]

def split_multi(s):
    if not s: return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(s)) if w.strip()]

def tokens_from_query(q):
    q = norm_key(q)
    return [t for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", q) if t]

def ensure_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df

def build_author_candidates(df: pd.DataFrame):
    rep = {}
    for v in df.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
        for name in split_authors(v):
            k = norm_key(name)
            if k and k not in rep:
                rep[k] = name
    return [rep[k] for k in sorted(rep.keys())]

def haystack(row):
    parts = [
        str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","")),
        str(row.get("è‘—è€…","")),
        str(row.get("file_name","")),
        " ".join(str(row.get(c,"")) for c in KEY_COLS if c in row),
    ]
    return norm_key(" \n ".join(parts))

def to_int_or_none(x):
    try: return int(str(x).strip())
    except Exception:
        m = re.search(r"\d+", str(x))
        return int(m.group()) if m else None

def order_by_template(values, template):
    vs = list(dict.fromkeys(values))
    tmpl_set = set(template)
    head = [v for v in template if v in vs and "ãã®ä»–" not in v]
    mid  = sorted([v for v in vs if v not in tmpl_set and "ãã®ä»–" not in v])
    tail = [v for v in template if v in vs and "ãã®ä»–" in v] + \
           [v for v in vs if ("ãã®ä»–" in v and v not in template)]
    return head + mid + tail

def make_visible_cols(df: pd.DataFrame) -> list[str]:
    base_hide = {"ç›¸å¯¾PASS", "çµ‚äº†ãƒšãƒ¼ã‚¸", "file_path", "num_pages", "file_name"}
    cols = [str(c) for c in df.columns]
    hide = set(c for c in cols if c in base_hide)
    if "llm_keywords" in cols:
        idx = cols.index("llm_keywords")
        hide.update(cols[idx:])
    return [c for c in cols if c not in hide]

def make_row_id(row):
    no = str(row.get("No.", "")).strip()
    if no and no.lower() not in {"none", "nan"}:
        return f"NO:{no}"
    ttl = str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«", "")).strip()
    yr  = str(row.get("ç™ºè¡Œå¹´", "")).strip()
    return f"T:{ttl}|Y:{yr}"

# -------------------- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå¾“æ¥ï¼‰ --------------------
st.title("é†¸é€ å”ä¼šèªŒã€€è«–æ–‡æ¤œç´¢ / åˆ†æ")

DEMO_CSV_PATH = Path("data/keywords_summary5.csv")   # ãƒ¡ã‚¤ãƒ³CSV
SUMMARY_CSV_PATH = Path("data/summaries.csv")         # summaryï¼ˆfile_name, summaryï¼‰
AUTHORS_CSV_PATH = Path("data/authors_readings.csv")  # è‘—è€…èª­ã¿ï¼ˆauthor, readingï¼‰
SECRET_URL = st.secrets.get("GSHEET_CSV_URL", "")

@st.cache_data(ttl=600, show_spinner=False)
def load_local_csv(path: Path) -> pd.DataFrame:
    return ensure_cols(pd.read_csv(path, encoding="utf-8"))

@st.cache_data(ttl=600, show_spinner=False)
def load_url_csv(url: str) -> pd.DataFrame:
    r = requests.get(url, timeout=30); r.raise_for_status()
    return pd.read_csv(io.BytesIO(r.content), encoding="utf-8")

@st.cache_data(ttl=600, show_spinner=False)
def load_summaries(path: Path) -> pd.DataFrame | None:
    try:
        if not path.exists(): return None
        df_s = pd.read_csv(path, encoding="utf-8")
        df_s.columns = [str(c).strip() for c in df_s.columns]
        if not {"file_name", "summary"}.issubset(df_s.columns): return None
        return df_s[["file_name", "summary"]]
    except Exception:
        return None

@st.cache_data(ttl=600, show_spinner=False)
def load_authors_readings(path: Path) -> pd.DataFrame | None:
    try:
        if not path.exists(): return None
        adf = pd.read_csv(path, encoding="utf-8")
        adf.columns = [str(c).strip() for c in adf.columns]
        if not {"author", "reading"}.issubset(adf.columns): return None
        adf["author"]  = adf["author"].astype(str).str.strip()
        adf["reading"] = adf["reading"].astype(str).str.strip()
        adf = adf[(adf["author"]!="") & (adf["reading"]!="")]
        adf = adf.drop_duplicates(subset=["reading"], keep="first")
        return adf
    except Exception:
        return None

with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    st.caption("â€» ã¾ãšã¯ãƒ‡ãƒ¢CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã€‚URL/ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šã§ä¸Šæ›¸ãã§ãã¾ã™ã€‚")
    use_demo = st.toggle("ãƒ‡ãƒ¢CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹", value=True)
    url = st.text_input("å…¬é–‹CSVã®URLï¼ˆGoogleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ output=csvï¼‰", value=SECRET_URL)
    up  = st.file_uploader("CSVã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿", type=["csv"])
    load_clicked = st.button("èª­ã¿è¾¼ã¿ï¼ˆURL/ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆï¼‰", type="primary", key="load_btn")

df = None; err = None
try:
    if load_clicked:
        if up is not None:
            df = ensure_cols(pd.read_csv(up, encoding="utf-8")); st.toast("ãƒ­ãƒ¼ã‚«ãƒ«CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        elif url.strip():
            df = load_url_csv(url.strip()); st.toast("URLã®CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            st.warning("URL ã¾ãŸã¯ CSV ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    elif use_demo and DEMO_CSV_PATH.exists():
        df = load_local_csv(DEMO_CSV_PATH); st.caption(f"âœ… ãƒ‡ãƒ¢CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ä¸­: {DEMO_CSV_PATH}")
    elif SECRET_URL:
        df = load_url_csv(SECRET_URL); st.caption("âœ… Secretsã®URLã‹ã‚‰è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ä¸­")
except Exception as e:
    err = e

if df is None:
    if err: st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {err}")
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ CSV ã‚’æŒ‡å®šã™ã‚‹ã‹ã€ãƒ‡ãƒ¢CSVã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# summary ãƒãƒ¼ã‚¸
sum_df = load_summaries(SUMMARY_CSV_PATH)
if sum_df is not None:
    df = df.merge(sum_df, on="file_name", how="left")

# è‘—è€…èª­ã¿å€™è£œï¼ˆå…¨ä½“ï¼‰ã‚’å…ˆã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ session ã«ä¿æŒï¼ˆæ¤œç´¢/åˆ†æã§å…±ç”¨ï¼‰
if "author_candidates" not in st.session_state:
    st.session_state.author_candidates = load_authors_readings(AUTHORS_CSV_PATH)

# -------------------- å…±é€šï¼šãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°ï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ --------------------
def apply_filters_for_current_state(_df: pd.DataFrame) -> pd.DataFrame:
    # ä¸‹è¨˜ã‚­ãƒ¼ã¯æ¤œç´¢ã‚¿ãƒ–ã§è¨­å®šï¼ˆã“ã“ã§ã¯å­˜åœ¨ã—ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‰±ã„ï¼‰
    y_from = st.session_state.get("y_from", None)
    y_to   = st.session_state.get("y_to", None)
    vols_sel   = st.session_state.get("vols_sel", [])
    issues_sel = st.session_state.get("issues_sel", [])
    authors_sel = st.session_state.get("authors_sel", [])
    targets_sel = st.session_state.get("targets_sel", [])
    types_sel   = st.session_state.get("types_sel", [])
    kw_query = st.session_state.get("kw_query", "")
    kw_mode  = st.session_state.get("kw_mode", "OR")

    df2 = _df.copy()
    # å¹´
    if y_from is not None and y_to is not None and "ç™ºè¡Œå¹´" in df2.columns:
        y = pd.to_numeric(df2["ç™ºè¡Œå¹´"], errors="coerce")
        df2 = df2[(y >= y_from) & (y <= y_to) | y.isna()]
    # å·»ãƒ»å·
    if vols_sel and "å·»æ•°" in df2.columns:
        df2 = df2[df2["å·»æ•°"].map(to_int_or_none).isin(set(vols_sel))]
    if issues_sel and "å·æ•°" in df2.columns:
        df2 = df2[df2["å·æ•°"].map(to_int_or_none).isin(set(issues_sel))]
    # è‘—è€…
    if authors_sel and "è‘—è€…" in df2.columns:
        sel = {norm_key(a) for a in authors_sel}
        def hit_author(v): return any(norm_key(x) in sel for x in split_authors(v))
        df2 = df2[df2["è‘—è€…"].apply(hit_author)]
    # å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆtop3åˆ—ã«å¯¾ã—ã¦éƒ¨åˆ†ä¸€è‡´ï¼‰
    if targets_sel and "å¯¾è±¡ç‰©_top3" in df2.columns:
        t_norm = [norm_key(t) for t in targets_sel]
        df2 = df2[df2["å¯¾è±¡ç‰©_top3"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]
    if types_sel and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in df2.columns:
        t_norm = [norm_key(t) for t in types_sel]
        df2 = df2[df2["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    toks = tokens_from_query(kw_query)
    if toks:
        def hit_kw(row):
            hs = haystack(row)
            return all(t in hs for t in toks) if kw_mode == "AND" else any(t in hs for t in toks)
        df2 = df2[df2.apply(hit_kw, axis=1)]
    return df2

# -------------------- ã‚¿ãƒ– --------------------
tab_search, tab_analysis = st.tabs(["ğŸ” æ¤œç´¢", "ğŸ“Š åˆ†æ"])

# ==================== ğŸ” æ¤œç´¢ã‚¿ãƒ– ====================
with tab_search:
    # -------------------- å¹´ãƒ»å·»ãƒ»å·ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¾“æ¥UIï¼‰ --------------------
    st.subheader("æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿")
    year_vals = pd.to_numeric(df.get("ç™ºè¡Œå¹´", pd.Series(dtype=str)), errors="coerce")
    if year_vals.notna().any():
        ymin_all, ymax_all = int(year_vals.min()), int(year_vals.max())
    else:
        ymin_all, ymax_all = 1980, 2025

    c_y, c_v, c_i = st.columns([1, 1, 1])
    with c_y:
        y_from, y_to = st.slider("ç™ºè¡Œå¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin_all, max_value=ymax_all, value=(ymin_all, ymax_all))
        st.session_state.y_from, st.session_state.y_to = y_from, y_to
    with c_v:
        vol_candidates = sorted({v for v in (df.get("å·»æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
        vols_sel = st.multiselect("å·»ï¼ˆè¤‡æ•°é¸æŠï¼‰", vol_candidates, default=[])
        st.session_state.vols_sel = vols_sel
    with c_i:
        iss_candidates = sorted({v for v in (df.get("å·æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
        issues_sel = st.multiselect("å·ï¼ˆè¤‡æ•°é¸æŠï¼‰", iss_candidates, default=[])
        st.session_state.issues_sel = issues_sel

    # ---- 1æ®µç›®ï¼šå¯¾è±¡ç‰© / ç ”ç©¶ã‚¿ã‚¤ãƒ— ----
    row1_tg, row1_tp = st.columns([1.2, 1.2])
    with row1_tg:
        raw_targets = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        targets_all = order_by_template(list(raw_targets), TARGET_ORDER)
        targets_sel = st.multiselect("å¯¾è±¡ç‰©ï¼ˆè¤‡æ•°é¸æŠï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", targets_all, default=[])
        st.session_state.targets_sel = targets_sel
    with row1_tp:
        raw_types = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        types_all = order_by_template(list(raw_types), TYPE_ORDER)
        types_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆè¤‡æ•°é¸æŠï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", types_all, default=[])
        st.session_state.types_sel = types_sel

    # ---- 2æ®µç›®ï¼šè‘—è€…ï¼ˆãƒ©ã‚¸ã‚ªï¼‹ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼‰ ----
    if "authors_sel" not in st.session_state:
        st.session_state.authors_sel = []

    def handle_author_multiselect_change():
        selected_readings = st.session_state.authors_multiselect_key
        reading2author = dict(zip(st.session_state.author_candidates["reading"], st.session_state.author_candidates["author"]))
        st.session_state.authors_sel = sorted({reading2author[r] for r in selected_readings}) if selected_readings else []
        st.rerun()

    row2_author, row2_radio = st.columns([1.0, 2.0])

    with row2_author:
        adf = st.session_state.author_candidates
        if adf is not None and not adf.empty:
            cand = adf.copy()

            GOJUON = {
                "ã‚": "ã‚ã„ã†ãˆãŠ",
                "ã‹": "ã‹ããã‘ã“ãŒããã’ã”",
                "ã•": "ã•ã—ã™ã›ãã–ã˜ãšãœã",
                "ãŸ": "ãŸã¡ã¤ã¦ã¨ã ã¢ã¥ã§ã©",
                "ãª": "ãªã«ã¬ã­ã®",
                "ã¯": "ã¯ã²ãµã¸ã»ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½",
                "ã¾": "ã¾ã¿ã‚€ã‚ã‚‚",
                "ã‚„": "ã‚„ã‚†ã‚ˆ",
                "ã‚‰": "ã‚‰ã‚Šã‚‹ã‚Œã‚",
                "ã‚": "ã‚ã‚’ã‚“",
            }
            def kata_to_hira(s: str) -> str:
                out = []
                for ch in str(s or ""):
                    o = ord(ch)
                    if 0x30A1 <= o <= 0x30F6: out.append(chr(o - 0x60))
                    else: out.append(ch)
                return "".join(out)
            def hira_head(s: str) -> str | None:
                s = str(s or ""); return kata_to_hira(s)[0] if s else None
            def is_roman_head(s: str) -> bool:
                return bool(re.match(r"[A-Za-z]", str(s or "")))

            ini = st.session_state.get("author_initial", "ã™ã¹ã¦")
            if ini == "è‹±å­—":
                cand = cand[cand["reading"].astype(str).str.match(r"[A-Za-z]")]
            elif ini != "ã™ã¹ã¦":
                allowed = set(GOJUON.get(ini, ""))
                cand = cand[cand["reading"].apply(
                    lambda s: (not is_roman_head(s)) and (hira_head(s) in allowed if hira_head(s) else False)
                )]

            AIUEO_ORDER = "ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“"
            def sort_tuple(reading: str):
                if not reading: return (3, 999, "")
                ch = reading[0]
                if re.match(r"[A-Za-z]", ch): return (2, 999, ch.lower())
                if re.match(r"[\u30A0-\u30FF]", ch): return (1, 999, reading)
                idx = AIUEO_ORDER.find(ch)
                return (0, idx if idx != -1 else 998, reading)

            cand = cand.assign(
                _grp=[sort_tuple(r)[0] for r in cand["reading"]],
                _key=[sort_tuple(r)[1] for r in cand["reading"]],
                _sub=[sort_tuple(r)[2] for r in cand["reading"]],
            ).sort_values(by=["_grp","_key","_sub"], kind="mergesort").drop(columns=["_grp","_key","_sub"])

            reading2author = dict(zip(cand["reading"], cand["author"]))
            options_readings = list(reading2author.keys())
            selected_readings = []
            if 'authors_sel' in st.session_state:
                selected_author_names = set(st.session_state.authors_sel)
                for r, a in reading2author.items():
                    if a in selected_author_names:
                        selected_readings.append(r)

            st.caption("è‘—è€…ã®èª­ã¿é ­æ–‡å­—ã§ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚’çµã‚Šè¾¼ã¿")
            authors_sel_readings = st.multiselect(
                "è‘—è€…ï¼ˆèª­ã¿ã§æ¤œç´¢å¯ / è¡¨ç¤ºã¯æ¼¢å­—ï¼‹èª­ã¿ï¼‰",
                options=options_readings,
                default=selected_readings,
                format_func=lambda r: f"{reading2author.get(r, r)}ï½œ{r}",
                placeholder="ä¾‹ï¼šã‚„ã¾ã  / ã•ã¨ã† / ãŸã‹ã¯ã— ...",
                on_change=handle_author_multiselect_change,
                key="authors_multiselect_key"
            )
        else:
            authors_all = build_author_candidates(df)
            st.session_state.authors_sel = st.multiselect(
                "è‘—è€…", authors_all, default=st.session_state.authors_sel
            )

    with row2_radio:
        initials = ["ã™ã¹ã¦","ã‚","ã‹","ã•","ãŸ","ãª","ã¯","ã¾","ã‚„","ã‚‰","ã‚","è‹±å­—"]
        if "author_initial" not in st.session_state:
            st.session_state.author_initial = "ã™ã¹ã¦"
        st.radio(
            "è‘—è€…ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«é¸æŠ",
            options=initials,
            horizontal=True,
            key="author_initial",
        )

    # ---- 3æ®µç›®ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ----
    kw_row1, kw_row2 = st.columns([3, 1])
    with kw_row1:
        kw_query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½/ã‚«ãƒ³ãƒã§è¤‡æ•°å¯ï¼‰", value="", key="kw_query")
    with kw_row2:
        st.session_state.kw_mode = st.radio("ä¸€è‡´æ¡ä»¶", ["OR", "AND"], index=0, horizontal=True, key="kw_mode")

    # -------------------- ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ã¨æ¤œç´¢çµæœ --------------------
    filtered = apply_filters_for_current_state(df)

    st.markdown("### æ¤œç´¢çµæœ")
    st.caption(f"{len(filtered)} / {len(df)} ä»¶")

    visible_cols = make_visible_cols(filtered)
    if "è‘—è€…" in visible_cols and "summary" in filtered.columns:
        idx = visible_cols.index("è‘—è€…")
        if "summary" not in visible_cols:
            visible_cols.insert(idx + 1, "summary")

    disp = filtered.loc[:, visible_cols].copy()
    disp["_row_id"] = disp.apply(make_row_id, axis=1)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼šãŠæ°—ã«å…¥ã‚Šé›†åˆï¼ã‚¿ã‚°è¾æ›¸
    if "favs" not in st.session_state:
        st.session_state.favs = set()
    if "fav_tags" not in st.session_state:
        st.session_state.fav_tags = {}

    disp["â˜…"] = disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)

    def handle_main_editor_change():
        edited_rows_dict = st.session_state.main_editor['edited_rows']
        for row_index_str, changes in edited_rows_dict.items():
            row_index = int(row_index_str)
            row_id = disp.iloc[row_index]['_row_id']
            if 'â˜…' in changes:
                if changes['â˜…']:
                    st.session_state.favs.add(row_id)
                else:
                    st.session_state.favs.discard(row_id)

    column_config = {
        "â˜…": st.column_config.CheckboxColumn("â˜…", help="æ°—ã«ãªã‚‹è«–æ–‡ã«ãƒã‚§ãƒƒã‚¯/è§£é™¤", default=False, width="small"),
    }
    if "HPãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
        column_config["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", help="å¤–éƒ¨ã‚µã‚¤ãƒˆã¸ç§»å‹•", display_text="HP")
    if "PDFãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
        column_config["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", help="PDFã‚’é–‹ã", display_text="PDF")

    display_order = ["â˜…"] + [c for c in disp.columns if c not in ["â˜…", "_row_id"]] + ["_row_id"]

    st.subheader("è«–æ–‡ãƒªã‚¹ãƒˆ")
    st.data_editor(
        disp[display_order],
        key="main_editor",
        on_change=handle_main_editor_change,
        use_container_width=True,
        hide_index=True,
        column_config=column_config,
        disabled=[c for c in display_order if c != "â˜…"],
        height=520,
        num_rows="fixed",
    )

    # --- ãŠæ°—ã«å…¥ã‚Šä¸€è¦§ ---
    c1, c2 = st.columns([6, 1])
    with c1:
        st.subheader(f"â­ ãŠæ°—ã«å…¥ã‚Šï¼ˆ{len(st.session_state.favs)} ä»¶ï¼‰")
    with c2:
        if st.button("âŒ å…¨ã¦å¤–ã™", key="clear_favs_header", use_container_width=True):
            st.session_state.favs = set()
            st.rerun()

    visible_cols_full = make_visible_cols(df)
    if "è‘—è€…" in visible_cols_full and "summary" in df.columns:
        idx = visible_cols_full.index("è‘—è€…")
        if "summary" not in visible_cols_full:
            visible_cols_full.insert(idx + 1, "summary")

    fav_disp_full = df.loc[:, visible_cols_full].copy()
    fav_disp_full["_row_id"] = fav_disp_full.apply(make_row_id, axis=1)
    fav_disp = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()

    def tags_str_for(rid: str) -> str:
        s = st.session_state.fav_tags.get(rid, set())
        return ", ".join(sorted(s)) if s else ""

    def update_fav_and_tags_from_favs():
        edited_favs = st.session_state.fav_editor['edited_rows']
        fav_ids_in_view = fav_disp['_row_id'].tolist()
        for row_index_str, changes in edited_favs.items():
            row_index = int(row_index_str)
            row_id = fav_ids_in_view[row_index]
            if 'â˜…' in changes:
                if changes['â˜…']:
                    st.session_state.favs.add(row_id)
                else:
                    st.session_state.favs.discard(row_id)
            if 'tags' in changes:
                tag_set = {t.strip() for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", str(changes['tags'])) if t.strip()}
                if tag_set:
                    st.session_state.fav_tags[row_id] = tag_set
                elif row_id in st.session_state.fav_tags:
                    del st.session_state.fav_tags[row_id]
        st.rerun()

    if not fav_disp.empty:
        fav_disp["â˜…"] = fav_disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)
        fav_disp["tags"] = fav_disp["_row_id"].apply(tags_str_for)

        fav_display_order = ["â˜…"] + [c for c in fav_disp.columns if c not in ["â˜…", "_row_id"]] + ["_row_id"]
        fav_column_config = {
            "â˜…": st.column_config.CheckboxColumn("â˜…", help="ãƒã‚§ãƒƒã‚¯ã§è§£é™¤/è¿½åŠ ", default=True, width="small"),
            "tags": st.column_config.TextColumn("tagsï¼ˆã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", help="ä¾‹: æ¸…é…’, ä¹³é…¸èŒ"),
        }
        if "HPãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
            fav_column_config["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", display_text="HP")
        if "PDFãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
            fav_column_config["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", display_text="PDF")

        st.data_editor(
            fav_disp[fav_display_order],
            key="fav_editor",
            on_change=update_fav_and_tags_from_favs,
            use_container_width=True,
            hide_index=True,
            column_config=fav_column_config,
            disabled=[c for c in fav_display_order if c not in ["â˜…", "tags"]],
            height=420,
            num_rows="fixed"
        )
    else:
        st.info("ãŠæ°—ã«å…¥ã‚Šã¯æœªé¸æŠã§ã™ã€‚ä¸Šã®è¡¨ã®ã€â˜…ã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰åæ˜ ã—ã¦ãã ã•ã„ã€‚")

    with st.expander("ğŸ” ã‚¿ã‚°ã§ãŠæ°—ã«å…¥ã‚Šã‚’çµã‚Šè¾¼ã¿ï¼ˆAND/ORï¼‰", expanded=False):
        tag_query = st.text_input("ã‚¿ã‚°æ¤œç´¢ï¼ˆã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key="tag_query")
        tag_mode = st.radio("ä¸€è‡´æ¡ä»¶", ["OR", "AND"], index=0, horizontal=True, key="tag_mode")
        fav_disp_for_filter = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()
        if tag_query.strip():
            tags = [t.strip() for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", tag_query) if t.strip()]
            def match_tags_row(row):
                row_tags = st.session_state.fav_tags.get(row["_row_id"], set())
                return all(t in row_tags for t in tags) if tag_mode == "AND" else any(t in row_tags for t in tags)
            fav_disp_for_filter = fav_disp_for_filter[fav_disp_for_filter.apply(match_tags_row, axis=1)]
        def tags_str_for_filter(rid: str) -> str:
            s = st.session_state.fav_tags.get(rid, set())
            return ", ".join(sorted(s)) if s else ""
        fav_disp_for_filter["tags"] = fav_disp_for_filter["_row_id"].apply(tags_str_for_filter)
        show_cols = ["No.","ç™ºè¡Œå¹´","å·»æ•°","å·æ•°","è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","è‘—è€…","å¯¾è±¡ç‰©_top3","ç ”ç©¶ã‚¿ã‚¤ãƒ—","HPãƒªãƒ³ã‚¯å…ˆ","PDFãƒªãƒ³ã‚¯å…ˆ","tags"]
        show_cols = [c for c in show_cols if c in fav_disp_for_filter.columns]
        st.dataframe(fav_disp_for_filter[show_cols], use_container_width=True, hide_index=True)

    st.caption(
        f"ç¾åœ¨ã®ãŠæ°—ã«å…¥ã‚Šï¼š{len(st.session_state.favs)} ä»¶ / "
        f"ã‚¿ã‚°æ•°ï¼š{len({t for s in st.session_state.fav_tags.values() for t in s})} ç¨€"
    )

    # å‡ºåŠ›
    filtered_export_df = disp.drop(columns=["â˜…", "_row_id"], errors="ignore")
    fav_export = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()
    def _tags_join(rid: str) -> str:
        s = st.session_state.fav_tags.get(rid, set())
        return ", ".join(sorted(s)) if s else ""
    fav_export["tags"] = fav_export["_row_id"].map(_tags_join)
    fav_export = fav_export.drop(columns=["_row_id"], errors="ignore")

    c_dl1, c_dl2 = st.columns(2)
    with c_dl1:
        st.download_button(
            "ğŸ“¥ çµã‚Šè¾¼ã¿çµæœã‚’CSVå‡ºåŠ›ï¼ˆè¡¨ç¤ºåˆ—ã®ã¿ï¼‰",
            data=filtered_export_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"filtered_{time.strftime('%Y%m%d')}.csv",
            mime="text/csv",
            use_container_width=True
        )
    with c_dl2:
        st.download_button(
            "â­ ãŠæ°—ã«å…¥ã‚Šã‚’CSVå‡ºåŠ›ï¼ˆtagsä»˜ãï¼‰",
            data=fav_export.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"favorites_{time.strftime('%Y%m%d')}.csv",
            mime="text/csv",
            use_container_width=True,
            disabled=fav_export.empty
        )

# ==================== ğŸ“Š åˆ†æã‚¿ãƒ– ====================
with tab_analysis:
    st.subheader("è‘—è€…å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆä¸­å¿ƒæ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»˜ãï¼‰")

    # å¯¾è±¡ãƒ‡ãƒ¼ã‚¿é¸æŠ
    scope = st.radio("å¯¾è±¡ãƒ‡ãƒ¼ã‚¿", ["æ¤œç´¢çµæœï¼ˆç¾åœ¨ã®æ¡ä»¶ï¼‰", "å…¨ä»¶"], horizontal=True)
    if scope == "æ¤œç´¢çµæœï¼ˆç¾åœ¨ã®æ¡ä»¶ï¼‰":
        df_scope = apply_filters_for_current_state(df)
    else:
        df_scope = df

    st.caption(f"å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼š{len(df_scope)}")

    # è‘—è€…ãƒªã‚¹ãƒˆæŠ½å‡º
    def authors_list_from_row(v):
        return split_authors(v) if isinstance(v, str) else []

    author_rows = df_scope.get("è‘—è€…", pd.Series(dtype=str)).fillna("").apply(authors_list_from_row)
    # ãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸
    G = nx.Graph()
    for authors in author_rows:
        authors = [a for a in authors if a]
        # è‡ªå·±ãƒ«ãƒ¼ãƒ—å›é¿ã€é‡è¤‡é™¤å»
        uniq = list(dict.fromkeys(authors))
        for a in uniq:
            if not G.has_node(a):
                G.add_node(a)
        for u, v in itertools.combinations(uniq, 2):
            if G.has_edge(u, v):
                G[u][v]["weight"] += 1
            else:
                G.add_edge(u, v, weight=1)

    if G.number_of_nodes() == 0:
        st.info("è‘—è€…ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¤œç´¢æ¡ä»¶ã‚’åºƒã’ã‚‹ã‹ã€å…¨ä»¶ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    else:
        # ä¸­å¿ƒæ€§
        deg_c = nx.degree_centrality(G)
        try:
            btw_c = nx.betweenness_centrality(G, weight="weight", normalized=True)
        except Exception:
            btw_c = nx.betweenness_centrality(G, normalized=True)
        try:
            eig_c = nx.eigenvector_centrality_numpy(G, weight="weight")
        except Exception:
            # fallbackï¼ˆåæŸã—ãªã„å ´åˆãªã©ï¼‰
            eig_c = {n: np.nan for n in G.nodes()}

        rank_df = pd.DataFrame({
            "è‘—è€…": list(G.nodes()),
            "Degree": [deg_c.get(n, 0.0) for n in G.nodes()],
            "Betweenness": [btw_c.get(n, 0.0) for n in G.nodes()],
            "Eigenvector": [eig_c.get(n, np.nan) for n in G.nodes()],
            "å…±åŒæ•°ï¼ˆç·è¨ˆï¼‰": [int(sum(d["weight"] for *_e, d in G.edges(n, data=True))) for n in G.nodes()]
        })
        # ä¸¦ã³æ›¿ãˆã‚­ãƒ¼ï¼ˆDegreeâ†’Betweennessâ†’Eigenvectorï¼‰
        rank_df = rank_df.sort_values(
            by=["Degree","Betweenness","Eigenvector","å…±åŒæ•°ï¼ˆç·è¨ˆï¼‰"],
            ascending=[False, False, False, False]
        ).reset_index(drop=True)

        topk = st.slider("ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºä»¶æ•°", min_value=10, max_value=200, value=50, step=10)
        st.dataframe(rank_df.head(topk), use_container_width=True, hide_index=True)

        st.markdown("---")
        st.markdown("#### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–")

        # PyVis â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ matplotlib
        if _HAS_PYVIS:
            nt = Network(height="680px", width="100%", notebook=False, bgcolor="#FFFFFF", font_color="#000000")
            # ç‰©ç†æ¼”ç®—
            nt.barnes_hut(gravity=-2000, central_gravity=0.3, spring_length=150, spring_strength=0.05, damping=0.9)

            # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºï¼ˆDegreeï¼‰
            max_deg = max(deg_c.values()) if deg_c else 1.0
            for n in G.nodes():
                size = 10 + 30 * (deg_c.get(n, 0) / max_deg if max_deg else 0)
                label = n
                title = f"{n}<br>Degree:{deg_c.get(n,0):.3f} / Bet:{btw_c.get(n,0):.3f} / Eig:{eig_c.get(n,0) if not math.isnan(eig_c.get(n,np.nan)) else 'NA'}"
                nt.add_node(n, label=label, title=title, value=size)

            for u, v, d in G.edges(data=True):
                w = d.get("weight", 1)
                nt.add_edge(u, v, value=w, title=f"å…±è‘—å›æ•°: {w}")

            html_file = "author_network.html"
            nt.show(html_file)
            with open(html_file, "r", encoding="utf-8") as f:
                html = f.read()
            st.components.v1.html(html, height=700, scrolling=True)
        else:
            # matplotlib ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            import matplotlib.pyplot as plt
            fig, ax = plt.subplots(figsize=(10, 8))
            pos = nx.spring_layout(G, k=0.6, seed=42, weight="weight")
            # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºï¼šDegree
            deg_vals = np.array([deg_c.get(n, 0.0) for n in G.nodes()])
            sizes = 100 + 1200 * (deg_vals / (deg_vals.max() if deg_vals.max() > 0 else 1))
            nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color="#69b3a2", alpha=0.8, ax=ax)
            # ã‚¨ãƒƒã‚¸ã¯é‡ã¿ã§å¤ªã•
            widths = [0.5 + 2.5 * (G[u][v].get("weight", 1) / max(1, max(nx.get_edge_attributes(G, "weight").values()))) for u,v in G.edges()]
            nx.draw_networkx_edges(G, pos, width=widths, alpha=0.4, ax=ax)
            nx.draw_networkx_labels(G, pos, font_size=8, ax=ax)
            ax.axis("off")
            st.pyplot(fig)